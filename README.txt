===== Motivation =====

Federated learning is a useful ML technique to train a large robust models
without needing to expose data to all parties involved. However due to the
heterogeneity in the data across parties the global model with have varied
perforamnce for different parties. Analyzing these performance differences
is key to ensuring that a global model works for all parties.

==== Design Goals ====

Analyize model bias for different parties due to variations in feature
distribution

==== Deliverables ====

- Understand and implement different FL techniques, Federated averagingm,
  Tilted Empirical Risk Minimization and Agnostic FL.

- Utilize a dataset for training and assessment of two FL techniques.

- Examine the enhancement in the variance of accuracy among individual
  client groups when employing various federated learning techniques.


=== System Blocks ====

???

===== HW/SW Reqs =====

Python, Laptop with CUDA-enabled GPU / Google Colab

==== Team Members ====

Samuel Chicoine - Setup, Writing and Research.

Nate Robinson - Software and Networking.

== Project Timeline ==

Sept 29th - Submit Proposal

Oct/Nov/Dec - Check ins
